distibution_train:
  backend: 'nccl' #  choices=['nccl', 'gloo', 'mpi', 'torch']
  init_method: 'env://'
  workers: 8
  
dataset:
  dataset: imagesquawk
  nclass: 10
  size: 128
  data_dir: '/root/autodl-tmp/imagenet/'
  load_memory: true
  batch_real: 1024
  nch: 3
network:
  net_type: convnet
  norm_type: instance
  depth: 5
  width: 1.0

train:
  evaluation_epochs: 2000
  epoch_print_freq: 10
  epoch_eval_interval: 100
  pertrain_epochs: 80
  batch_size: 128
  lr: 0.01
  adamw_lr: 0.001    
  eval_optimizer: adamw
  momentum: 0.9
  weight_decay: 5e-4
  seed: 0
  model_num: 20


augmentation:
  mixup: cut
  beta: 1.0
  mix_p: 0.5
  rrc: true
  dsa: true
  dsa_strategy: "color_crop_cutout_flip_scale_rotate"
  aug_type: 'color_crop_cutout'

optimization:
  optimizer: adamw
  lr_scale_adam: 0.1
  lr_img: 0.01 
  mom_img: 0.5
  lr_sampling_net: 1e-3

save_path:
  save_dir: "../results/condense"
  pretrain_dir: '../pretrained_models'

condense:
  ipc: 10
  num_premodel: 20
  niter: 20000
  iter_calib: 1 
  calib_weight: 1
  sampling_net: False
  num_freqs: 4096
  dis_metrics: "NCFM"
  factor: 2
  alpha_for_loss: 0.5
  beta_for_loss: 0.5
  decode_type: 'single' # choices=['single', 'multi', 'bound']
  teacher_model_epoch: 20